# @package _group_

common:
  fp16: false
  log_interval: 4
  wandb_project: SL_TD_pose

task:
  _name: SL_topic_detection
  feats_type: ???
  body_parts: UPPER_BODY,LEFT_HAND_LANDMARKS,RIGHT_HAND_LANDMARKS
  feat_dims: "0,1"
  data: ???
  dict_path: ???
  normalization: layer_norm
  min_source_positions: 150
  max_source_positions: 250000
  max_target_positions: 1
  dataset: ''

dataset:
  num_workers: 0
  batch_size: 16
  skip_invalid_size_inputs_valid_test: true
  train_subset: train
  valid_subset: val
  validate_interval_updates: 600

#criterion:
#  _name: label_smoothed_cross_entropy_weighted
#  class_weights: "0.032935,0.033554,0.131136,0.103578,0.052167,0.072557,0.051789,0.111670,0.255246,0.155367"

#criterion:
#  _name: focal_loss
#  alpha: "0.032935,0.033554,0.131136,0.103578,0.052167,0.072557,0.051789,0.111670,0.255246,0.155367"
#  gamma: 3.0  # Default value is 2, controls the focusing factor, higher values focus more on hard examples.

#criterion:
#  _name: focal_loss
#  alpha: "1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0"
#  gamma: 2

criterion:
  _name: focal_loss
  alpha: "0.032935,0.033554,0.131136,0.103578,0.052167,0.072557,0.051789,0.111670,0.255246,0.155367"
  gamma: 2.0  # Default value is 2, controls the focusing factor, higher values focus more on hard examples.


optimization:
  max_update: 3000 #####
  lr: [0.0001]
  update_freq: [1]

optimizer:
  _name: adam
  adam_betas: [0.9, 0.998]
  weight_decay: 0.001

lr_scheduler:
  _name: inverse_sqrt
  warmup_updates: 500

checkpoint:
  keep_last_epochs: 1
  best_checkpoint_metric: acc
  maximize_best_checkpoint_metric: true
  save_dir: ???

model:
  _name: SL_topic_detection_transformer_CLS
  subsample_input: true
  #apply_mask: false
  dropout: 0.0
  #feature_grad_mult: 0.0
  encoder_embed_dim: 128
  encoder_ffn_embed_dim: 1024
  #freeze_finetune_updates: 0
  encoder_attention_heads: 4
  encoder_layers: 4

bpe:
  _name: sentencepiece
  sentencepiece_model: ???
